{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7943996,"sourceType":"datasetVersion","datasetId":4670796},{"sourceId":7969286,"sourceType":"datasetVersion","datasetId":4682715,"isSourceIdPinned":true}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install MMSegmentation¶","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-26T06:59:36.894405Z","iopub.execute_input":"2024-03-26T06:59:36.895263Z"}}},{"cell_type":"code","source":"!pip install torch==2.0.0\n!pip install openmim","metadata":{"execution":{"iopub.status.busy":"2024-03-29T15:40:34.228811Z","iopub.execute_input":"2024-03-29T15:40:34.229503Z","iopub.status.idle":"2024-03-29T15:43:53.256358Z","shell.execute_reply.started":"2024-03-29T15:40:34.229469Z","shell.execute_reply":"2024-03-29T15:43:53.255089Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torch==2.0.0\n  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (3.1.2)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.0.0 (from torch==2.0.0)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (69.0.3)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.42.0)\nCollecting cmake (from triton==2.0.0->torch==2.0.0)\n  Downloading cmake-3.29.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\nCollecting lit (from triton==2.0.0->torch==2.0.0)\n  Downloading lit-18.1.2.tar.gz (161 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.0/161.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.0) (1.3.0)\nDownloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cmake-3.29.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: lit\n  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-18.1.2-py3-none-any.whl size=96368 sha256=629dacf6928348686725874cc189e74c054da6ae64a4c75bfa502f9b62cc1272\n  Stored in directory: /root/.cache/pip/wheels/f4/4d/9c/3e28d23c2c6fc6a9bd89c91a7b7ff775fc71a41ac9a52563e9\nSuccessfully built lit\nInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, cmake, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\nSuccessfully installed cmake-3.29.0.1 lit-18.1.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 triton-2.0.0\nCollecting openmim\n  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: Click in /opt/conda/lib/python3.10/site-packages (from openmim) (8.1.7)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from openmim) (0.4.6)\nCollecting model-index (from openmim)\n  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\nCollecting opendatalab (from openmim)\n  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from openmim) (2.1.4)\nRequirement already satisfied: pip>=19.3 in /opt/conda/lib/python3.10/site-packages (from openmim) (23.3.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from openmim) (2.31.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from openmim) (13.7.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from openmim) (0.9.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from model-index->openmim) (6.0.1)\nRequirement already satisfied: markdown in /opt/conda/lib/python3.10/site-packages (from model-index->openmim) (3.5.2)\nRequirement already satisfied: ordered-set in /opt/conda/lib/python3.10/site-packages (from model-index->openmim) (4.1.0)\nRequirement already satisfied: pycryptodome in /opt/conda/lib/python3.10/site-packages (from opendatalab->openmim) (3.20.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from opendatalab->openmim) (4.66.1)\nCollecting openxlab (from opendatalab->openmim)\n  Downloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->openmim) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->openmim) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->openmim) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->openmim) (2024.2.2)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas->openmim) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->openmim) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->openmim) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->openmim) (2023.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->openmim) (1.16.0)\nCollecting oss2~=2.17.0 (from openxlab->opendatalab->openmim)\n  Downloading oss2-2.17.0.tar.gz (259 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting requests (from openmim)\n  Downloading requests-2.28.2-py3-none-any.whl.metadata (4.6 kB)\nCollecting rich (from openmim)\n  Downloading rich-13.4.2-py3-none-any.whl.metadata (18 kB)\nCollecting setuptools~=60.2.0 (from openxlab->opendatalab->openmim)\n  Downloading setuptools-60.2.0-py3-none-any.whl.metadata (5.1 kB)\nCollecting tqdm (from opendatalab->openmim)\n  Downloading tqdm-4.65.2-py3-none-any.whl.metadata (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: crcmod>=1.7 in /opt/conda/lib/python3.10/site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (1.7)\nCollecting aliyun-python-sdk-kms>=2.4.1 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n  Downloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting aliyun-python-sdk-core>=2.13.12 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n  Downloading aliyun-python-sdk-core-2.15.0.tar.gz (443 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.1/443.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim)\n  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: cryptography>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (41.0.7)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.21)\nDownloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading model_index-0.1.11-py3-none-any.whl (34 kB)\nDownloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\nDownloading openxlab-0.0.37-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests-2.28.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rich-13.4.2-py3-none-any.whl (239 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm-4.65.2-py3-none-any.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading setuptools-60.2.0-py3-none-any.whl (953 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.1/953.1 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: oss2, aliyun-python-sdk-core\n  Building wheel for oss2 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for oss2: filename=oss2-2.17.0-py3-none-any.whl size=112371 sha256=d246f9c1baf661e8969239a85aa4be453df5801018e6a20fed452f1f6a67ad07\n  Stored in directory: /root/.cache/pip/wheels/87/04/7b/7e61b8157fdf211c5131375240d0d86ca82e2a88ead9672c88\n  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.15.0-py3-none-any.whl size=535312 sha256=e58f4dff409a05733409552f5ed4aa2ad55f04f10103d43daef0441b118dc4e5\n  Stored in directory: /root/.cache/pip/wheels/b7/28/7c/a888bb3c60c865d014c7ef5017c83fdbc1cb0f601b79c7794a\nSuccessfully built oss2 aliyun-python-sdk-core\nInstalling collected packages: tqdm, setuptools, requests, model-index, jmespath, rich, aliyun-python-sdk-core, aliyun-python-sdk-kms, oss2, openxlab, opendatalab, openmim\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.66.1\n    Uninstalling tqdm-4.66.1:\n      Successfully uninstalled tqdm-4.66.1\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 69.0.3\n    Uninstalling setuptools-69.0.3:\n      Successfully uninstalled setuptools-69.0.3\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: jmespath\n    Found existing installation: jmespath 1.0.1\n    Uninstalling jmespath-1.0.1:\n      Successfully uninstalled jmespath-1.0.1\n  Attempting uninstall: rich\n    Found existing installation: rich 13.7.0\n    Uninstalling rich-13.7.0:\n      Successfully uninstalled rich-13.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.5 which is incompatible.\nboto3 1.26.100 requires botocore<1.30.0,>=1.29.100, but you have botocore 1.34.51 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2024.3.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-pubsub 2.19.0 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\njupyterlab-server 2.25.2 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aliyun-python-sdk-core-2.15.0 aliyun-python-sdk-kms-2.16.2 jmespath-0.10.0 model-index-0.1.11 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.37 oss2-2.17.0 requests-2.28.2 rich-13.4.2 setuptools-60.2.0 tqdm-4.65.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!mim install mmengine\n!mim install mmcv>=2.0.0\n!pip install mmsegmentation>=1.0.0\n!pip install ftfy ","metadata":{"execution":{"iopub.status.busy":"2024-03-29T15:52:48.375377Z","iopub.execute_input":"2024-03-29T15:52:48.376147Z","iopub.status.idle":"2024-03-29T15:54:08.133648Z","shell.execute_reply.started":"2024-03-29T15:52:48.376111Z","shell.execute_reply":"2024-03-29T15:54:08.130963Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n  warnings.warn(\nLooking in links: https://download.openmmlab.com/mmcv/dist/cu117/torch2.0.0/index.html\nCollecting mmengine\n  Downloading mmengine-0.10.3-py3-none-any.whl.metadata (20 kB)\nCollecting addict (from mmengine)\n  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mmengine) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mmengine) (1.26.4)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from mmengine) (6.0.1)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from mmengine) (13.4.2)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from mmengine) (2.4.0)\nRequirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from mmengine) (0.40.2)\nRequirement already satisfied: opencv-python>=3 in /opt/conda/lib/python3.10/site-packages (from mmengine) (4.9.0.80)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (2.9.0.post0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->mmengine) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->mmengine) (2.17.2)\nRequirement already satisfied: importlib-metadata>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine) (6.11.0)\nRequirement already satisfied: platformdirs>=3.5.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine) (4.2.0)\nRequirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine) (2.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.6.0->yapf->mmengine) (3.17.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->mmengine) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mmengine) (1.16.0)\nDownloading mmengine-0.10.3-py3-none-any.whl (451 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.7/451.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nInstalling collected packages: addict, mmengine\nSuccessfully installed addict-2.4.0 mmengine-0.10.3\n/opt/conda/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n  warnings.warn(\nCollecting ftfy\n  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\nDownloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# !git clone -b main https://github.com/open-mmlab/mmsegmentation.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !cd mmsegmentation && pip install -e .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mim download mmsegmentation --config ocrnet_hr18_4xb2-160k_cityscapes-512x1024 --dest .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport mmseg\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"","metadata":{"execution":{"iopub.status.busy":"2024-03-28T22:33:10.967831Z","iopub.status.idle":"2024-03-28T22:33:10.968163Z","shell.execute_reply.started":"2024-03-28T22:33:10.968005Z","shell.execute_reply":"2024-03-28T22:33:10.968018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mmseg.apis import inference_model, init_model, show_result_pyplot\nimport mmcv\n\nconfig_file = '/kaggle/working/ocrnet_hr18_4xb2-160k_cityscapes-512x1024.py'\ncheckpoint_file = '/kaggle/working/ocrnet_hr18_512x1024_160k_cityscapes_20200602_191001-b9172d0c.pth'\n\n# build the model from a config file and a checkpoint file\nmodel = init_model(config_file, checkpoint_file, device='cpu')\n\n# test a single image and show the results\nimg = '/kaggle/working/mmsegmentation/demo/demo.png'  # or img = mmcv.imread(img), which will only load it once\nresult = inference_model(model, img)\n# visualize the results in a new window\nshow_result_pyplot(model, img, result, show=True)\n# or save the visualization results to image files\n# you can change the opacity of the painted segmentation map in (0, 1].\nshow_result_pyplot(model, img, result, show=True, out_file='result.jpg', opacity=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T22:33:10.969801Z","iopub.status.idle":"2024-03-28T22:33:10.970133Z","shell.execute_reply.started":"2024-03-28T22:33:10.969973Z","shell.execute_reply":"2024-03-28T22:33:10.969987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and prepare data for training","metadata":{}},{"cell_type":"code","source":"from glob import glob\nimport cv2\nimport numpy as np\nimport xml.etree.ElementTree as ET\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nsns.set_theme()\nimport pandas as pd\nimport os\nimport json\n\n# ================================================================================================\n\nimport platform\nimport sys","metadata":{"execution":{"iopub.status.busy":"2024-03-29T15:54:08.136071Z","iopub.execute_input":"2024-03-29T15:54:08.136432Z","iopub.status.idle":"2024-03-29T15:54:09.974145Z","shell.execute_reply.started":"2024-03-29T15:54:08.136400Z","shell.execute_reply":"2024-03-29T15:54:09.973295Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# !mkdir /kaggle/working/OR_lab2/_models\n# !mkdir /kaggle/working/OR_lab2/_figures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---------------------------------------------------------------------------\nroot = \"/kaggle/input/fashionpedia-or/fashionpedia\"\nworking_dir = \"/kaggle/working\"\n# ---------------------------------------------------------------------------\n# Set models and figures directories\nmodels_dir = \"/kaggle/working/OR_lab2/_models\"\nfigures_dir = \"/kaggle/working/OR_lab2/_figures\"\n# ---------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-03-29T15:54:09.975486Z","iopub.execute_input":"2024-03-29T15:54:09.975922Z","iopub.status.idle":"2024-03-29T15:54:09.981247Z","shell.execute_reply.started":"2024-03-29T15:54:09.975894Z","shell.execute_reply":"2024-03-29T15:54:09.979969Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def load_annotations(annotation_file):\n    with open(annotation_file) as f:\n        data = json.load(f)\n        \n    images = pd.DataFrame(data['images'])\n    annotations = pd.DataFrame(data['annotations'])\n    categories = pd.DataFrame(data['categories'])\n    \n    return images, annotations, categories\n\ndef aggregate_rellevant_training_data(group):\n    category_ids = []\n    segmentations = []\n    file_name = \"\"\n    \n    for _, row in group.iterrows():\n        category_ids.append(row['category_id'])\n        segmentation = row['segmentation']\n        if isinstance(segmentation, list):  # If polygon encoded\n            segmentations.extend(segmentation)\n        elif isinstance(segmentation, dict):  # If RLE encoded\n            segmentations.append(segmentation)\n        file_name = row['file_name']\n\n    result_df = pd.Series({'image_id': group['image_id'].iloc[0], 'categories_ids': category_ids, 'segmentations': segmentations, 'file_name': file_name})\n    return result_df\n\ndef plot_raw_segmented_image(image_info, image_path, figsize=(15, 15)):\n    image = cv2.imread(image_path)\n    masked_image = image.copy()\n\n    encoded_pixels = image_info[\"segmentations\"]\n    class_ids = image_info[\"categories_ids\"]\n\n    # Plot the original image\n    fig, axs = plt.subplots(1, 2, figsize=figsize)\n    axs[0].imshow(image)\n\n    mask = np.zeros_like(image[:, :, 0])\n    for pixels, class_id in zip(encoded_pixels, class_ids):\n        # Create a mask for the current segment\n        vertices = np.array(pixels).reshape((-1, 2)).astype(np.int32)\n        cv2.fillPoly(mask, [vertices], 255 - class_id * 4)\n\n    axs[1].imshow(image)\n    axs[1].imshow(mask, alpha=0.8)\n\n    plt.show()\n    \n    \ndef save_raw_segmented_image(image_info, image_path, output_dir):\n    image = cv2.imread(image_path)\n    masked_image = image.copy()\n\n    encoded_pixels = image_info[\"segmentations\"]\n    class_ids = image_info[\"categories_ids\"]\n\n    mask = np.zeros_like(image[:, :, 0])\n    for pixels, class_id in zip(encoded_pixels, class_ids):\n        # Create a mask for the current segment\n        vertices = np.array(pixels).reshape((-1, 2)).astype(np.int32)\n        cv2.fillPoly(mask, [vertices], 255 - class_id * 4)\n\n    # Apply the mask to the image\n    masked_image[mask > 0] = [0, 255, 0]  # Set mask region to green (you can change color if needed)\n\n    # Save the masked image\n    filename = os.path.basename(image_path)\n    output_path = os.path.join(output_dir, filename)\n    cv2.imwrite(output_path, masked_image)","metadata":{"execution":{"iopub.status.busy":"2024-03-29T15:54:09.983763Z","iopub.execute_input":"2024-03-29T15:54:09.984089Z","iopub.status.idle":"2024-03-29T15:54:10.001498Z","shell.execute_reply.started":"2024-03-29T15:54:09.984063Z","shell.execute_reply":"2024-03-29T15:54:10.000515Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(image_path):\n    train_transforms = transforms.Compose([\n        transforms.RandomRotation(degrees=20),  # Random rotation (±20 degrees)\n        transforms.RandomHorizontalFlip(),      # Random horizontal flip\n        transforms.RandomVerticalFlip(),        # Random vertical flip\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), \n        transforms.RandomResizedCrop(size=(height, width), scale=(0.8, 1.0)),  # Random resized crop\n        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=0.1),  # Random affine transformation\n        transforms.ToTensor(),                  # Convert image to tensor\n    ])\n    \n    img = mmcv.imread(image_path)\n    img = train_transforms(img)\n    return img\n\ndef inference_and_visualize(model, image_path, output_dir):\n    img = preprocess_image(image_path)\n    result = inference_model(model, img)\n    show_result_pyplot(model, img, result, show=False, out_file=os.path.join(output_dir, 'result.jpg'))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-29T15:54:10.002680Z","iopub.execute_input":"2024-03-29T15:54:10.002977Z","iopub.status.idle":"2024-03-29T15:54:10.016359Z","shell.execute_reply.started":"2024-03-29T15:54:10.002951Z","shell.execute_reply":"2024-03-29T15:54:10.015397Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_images, train_annotations, train_categories = load_annotations(root + \"/Annotations/instances_attributes_train2020.json\")\ntest_images, test_annotations, test_categories = load_annotations(root + \"/Annotations/instances_attributes_val2020.json\")\n\ntrain_merged_df = pd.merge(train_annotations, train_images, left_on='image_id', right_on='id', how='outer')\ntrain_merged_df = train_merged_df.drop(columns=['id_x', 'id_y', 'license', 'time_captured', 'isstatic', 'original_url', 'iscrowd', 'kaggle_id'])\n\n\nrelevant_training_data = train_merged_df.groupby('image_id').apply(aggregate_rellevant_training_data)\nrelevant_training_data.reset_index(drop=True, inplace=True)\n\nrelevant_training_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-29T15:54:10.017794Z","iopub.execute_input":"2024-03-29T15:54:10.018135Z","iopub.status.idle":"2024-03-29T15:55:15.744400Z","shell.execute_reply.started":"2024-03-29T15:54:10.018109Z","shell.execute_reply":"2024-03-29T15:55:15.743402Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/28044364.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  relevant_training_data = train_merged_df.groupby('image_id').apply(aggregate_rellevant_training_data)\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   image_id                                     categories_ids  \\\n0        23                                   [23, 23, 33, 10]   \n1        25             [2, 33, 31, 31, 13, 7, 22, 22, 23, 23]   \n2        26  [13, 29, 28, 32, 32, 31, 31, 0, 31, 31, 18, 4,...   \n3        27  [6, 23, 23, 31, 31, 4, 1, 35, 32, 35, 35, 35, ...   \n4        28                        [10, 32, 35, 31, 4, 29, 33]   \n\n                                       segmentations  \\\n0  [[456, 970, 465, 979, 483, 982, 501, 983, 503,...   \n1  [[391, 233, 393, 233, 397, 233, 398, 233, 400,...   \n2  [[441, 135, 456, 139, 457, 143, 462, 148, 472,...   \n3  [[317, 752, 324, 755, 330, 755, 334, 754, 340,...   \n4  [[428, 1008, 420, 1016, 413, 1020, 406, 1022, ...   \n\n                              file_name  \n0  3ce385855f07c77fdeb911ed15094c53.jpg  \n1  97e45101f7235a9e56fa95c5e4980c17.jpg  \n2  47cbe3ead1617a9971dccc438a8e8884.jpg  \n3  361cc7654672860b1b7c85fe8e92b38a.jpg  \n4  8a20effd8b6ebcaf2b74caa7d35eee41.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>categories_ids</th>\n      <th>segmentations</th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23</td>\n      <td>[23, 23, 33, 10]</td>\n      <td>[[456, 970, 465, 979, 483, 982, 501, 983, 503,...</td>\n      <td>3ce385855f07c77fdeb911ed15094c53.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25</td>\n      <td>[2, 33, 31, 31, 13, 7, 22, 22, 23, 23]</td>\n      <td>[[391, 233, 393, 233, 397, 233, 398, 233, 400,...</td>\n      <td>97e45101f7235a9e56fa95c5e4980c17.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>[13, 29, 28, 32, 32, 31, 31, 0, 31, 31, 18, 4,...</td>\n      <td>[[441, 135, 456, 139, 457, 143, 462, 148, 472,...</td>\n      <td>47cbe3ead1617a9971dccc438a8e8884.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27</td>\n      <td>[6, 23, 23, 31, 31, 4, 1, 35, 32, 35, 35, 35, ...</td>\n      <td>[[317, 752, 324, 755, 330, 755, 334, 754, 340,...</td>\n      <td>361cc7654672860b1b7c85fe8e92b38a.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>[10, 32, 35, 31, 4, 29, 33]</td>\n      <td>[[428, 1008, 420, 1016, 413, 1020, 406, 1022, ...</td>\n      <td>8a20effd8b6ebcaf2b74caa7d35eee41.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# No es necesari ja qu es fa on the fly\n# output_directory = root + \"/masked_images\"\n# os.makedirs(output_directory, exist_ok=True)\n\n# train_images_path = root + \"/train\"\n# relevant_training_data.apply(lambda row: save_masked_image(row, os.path.join(train_images_path, row[\"file_name\"]), output_directory), axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom mmseg.apis import inference_model, init_model, show_result_pyplot\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nimport mmcv\nimport torch.nn.functional as F\n","metadata":{"execution":{"iopub.status.busy":"2024-03-29T18:54:22.472124Z","iopub.execute_input":"2024-03-29T18:54:22.472508Z","iopub.status.idle":"2024-03-29T18:54:22.479088Z","shell.execute_reply.started":"2024-03-29T18:54:22.472479Z","shell.execute_reply":"2024-03-29T18:54:22.477907Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"classes = ['pocket', 'sleeve', 'sock', 'collar', 'shirt, blouse', 'dress', 'shoe',\n           'tights, stockings', 'glasses', 'belt', 'bag, wallet', 'neckline',\n           'pants', 'hood', 'epaulette', 'coat', 'top, t-shirt, sweatshirt',\n           'zipper', 'hat', 'lapel', 'jacket', 'flower', 'ruffle', 'applique',\n           'skirt', 'buckle', 'scarf', 'glove', 'shorts', 'jumpsuit', 'bead',\n           'watch', 'tie', 'headband, head covering, hair accessory', 'umbrella',\n           'fringe', 'rivet', 'sweater', 'cardigan', 'vest', 'sequin', 'ribbon',\n           'bow', 'cape', 'tassel', 'leg warmer']\npalette = [255, 251, 247, 243, 239, 235, 231, 227, 223, 219, 215, 211, 207, 203, 199, 195, 191, 187, 183, 179, 175, 171, 167, 163, 159, 155, 151, 147, 143, 139, 135, 131, 127, 123, 119, 115, 111, 107, 103, 99, 95, 91, 87, 83, 79, 75]\n\n\ndef decode_rle(rle, class_id, image):\n    \"\"\"\n    Decode a run-length encoded mask.\n    \"\"\"\n    try:\n        mask = np.zeros_like(image[:, :, 0], dtype=np.uint8)\n        counts = rle[\"counts\"]\n        counts = counts.encode('utf-8')  # Encoding the string to bytes for correct parsing\n        counts = np.frombuffer(counts, dtype=np.uint8) - 48  # Convert ASCII codes to integer values\n   \n        starts = np.cumsum(counts[::2])\n        \n        min_length = min(len(starts), len(counts[1::2]))\n        starts = starts[:min_length]\n        ends = starts + counts[1::2]\n        ends = ends[:min_length]\n        \n        for start, end in zip(starts, ends):\n            mask[start:end] = 255 - class_id * 4\n        return mask\n    except Exception as e:\n        print(f\"Error decoding RLE: {str(e)}\")\n        raise e\n\nclass FashionpediaDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, data_df, root_dir, transform=None):\n        self.data_df = data_df\n        self.root_dir = root_dir\n        self.transform = transform\n        self.CLASSES = classes\n        self.PALETTE = palette\n\n    def __len__(self):\n        return len(self.data_df)\n\n    def __getitem__(self, idx):\n        try:\n            image_info = self.data_df.iloc[idx]\n            image_path = os.path.join(self.root_dir, image_info['file_name'])\n            image = cv2.imread(image_path)\n            encoded_segments = image_info[\"segmentations\"]\n            class_ids = image_info[\"categories_ids\"]\n            masks = {class_id: np.zeros_like(image[:, :, 0], dtype=np.uint8) for class_id in range(len(self.CLASSES))}\n\n            for segment, class_id in zip(encoded_segments, class_ids):\n                if isinstance(segment, dict):  # If RLE encoded\n                    segment_rle = decode_rle(segment, class_id,  image)\n                    masks[class_id] = segment_rle\n\n                else:  # If polygon encoded\n                    segment_pol = np.array(segment).reshape((-1, 2)).astype(np.int32)\n                    cv2.fillPoly(masks[class_id], [segment_pol], 255 - class_id * 4)\n\n            sample = {'image': image, 'masks': masks}\n\n            if self.transform:\n                sample = self.transform(sample)\n\n            return sample\n        except Exception as e:\n            if isinstance(segment, dict):\n                print(\"Segment:\", segment_rle.shape)\n            else:\n                print(\"Segment:\", segment_pol.shape)\n            print(f\"Error processing item {idx}: {str(e)}\")\n            raise Exception\n\n            \n    \nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image, masks = sample['image'], sample['masks']\n\n        # Swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        masks = {class_id: mask.transpose((0, 1)) for class_id, mask in masks.items()}\n\n        return {'image': torch.from_numpy(image), 'masks': masks}\n\nclass ApplyTransform(object):\n    def __init__(self, transform):\n        self.transform = transform\n\n    def __call__(self, sample):\n        image, masks = sample['image'], sample['masks']\n\n        # Apply transformation to both image and masks\n        image = self.transform(image)\n        masks = {class_id: self.transform(mask) for class_id, mask in masks.items()}\n\n        return {'image': image, 'masks': masks}","metadata":{"execution":{"iopub.status.busy":"2024-03-29T19:05:38.557794Z","iopub.execute_input":"2024-03-29T19:05:38.558699Z","iopub.status.idle":"2024-03-29T19:05:38.586349Z","shell.execute_reply.started":"2024-03-29T19:05:38.558666Z","shell.execute_reply":"2024-03-29T19:05:38.585371Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntrain_images_path = root + \"/train\"\nbatch_size = 8\nnum_epochs = 8\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nIMAGE_HEIGHT = 192\nIMAGE_WIDTH = 192\nnum_classes = 46\n\n# Define transformations\ntrain_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomRotation(degrees=20),\n    transforms.RandomResizedCrop(size=(IMAGE_HEIGHT, IMAGE_WIDTH)),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n    transforms.ToTensor(),\n])\n\n\ntrain_dataset = FashionpediaDataset(data_df=relevant_training_data,\n                              root_dir=train_images_path,\n                              transform=ApplyTransform(train_transforms))","metadata":{"execution":{"iopub.status.busy":"2024-03-29T19:05:41.915639Z","iopub.execute_input":"2024-03-29T19:05:41.915999Z","iopub.status.idle":"2024-03-29T19:05:41.924209Z","shell.execute_reply.started":"2024-03-29T19:05:41.915973Z","shell.execute_reply":"2024-03-29T19:05:41.923022Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"# Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# config_file = working_dir + '/ocrnet_hr18_4xb2-160k_cityscapes-512x1024.py'\nconfig_file = \"/kaggle/input/ocrnet-hr18-4xb2-160k-cityscapes-512x1024/ocrnet_hr18_4xb2-160k_cityscapes-512x1024.py\"\ncheckpoint_file = working_dir +'/ocrnet_hr18_512x1024_160k_cityscapes_20200602_191001-b9172d0c.pth'\n\nmodel = init_model(config_file, checkpoint_file, device=DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters())\nlog_interval = 10","metadata":{"execution":{"iopub.status.busy":"2024-03-29T19:05:43.922155Z","iopub.execute_input":"2024-03-29T19:05:43.922532Z","iopub.status.idle":"2024-03-29T19:05:44.548209Z","shell.execute_reply.started":"2024-03-29T19:05:43.922505Z","shell.execute_reply":"2024-03-29T19:05:44.547353Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"Loads checkpoint by local backend from path: /kaggle/working/ocrnet_hr18_512x1024_160k_cityscapes_20200602_191001-b9172d0c.pth\nThe model and loaded state dict do not match exactly\n\nsize mismatch for decode_head.0.conv_seg.weight: copying a param with shape torch.Size([19, 270, 1, 1]) from checkpoint, the shape in current model is torch.Size([46, 270, 1, 1]).\nsize mismatch for decode_head.0.conv_seg.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([46]).\nsize mismatch for decode_head.1.conv_seg.weight: copying a param with shape torch.Size([19, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([46, 512, 1, 1]).\nsize mismatch for decode_head.1.conv_seg.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([46]).\n","output_type":"stream"}]},{"cell_type":"code","source":"from mmcv import Config\nfrom mmseg.apis import set_random_seed\n\ncfg = Config.fromfile(config_file)\n\n\ncfg.norm_cfg = dict(type='BN', requires_grad=True)\ncfg.model.backbone.norm_cfg = cfg.norm_cfg\ncfg.model.decode_head.norm_cfg = cfg.norm_cfg\ncfg.model.auxiliary_head.norm_cfg = cfg.norm_cfg\n\ncfg.model.decode_head.num_classes = 46\ncfg.model.auxiliary_head.num_classes = 46\n\ncfg.dataset_type = 'FashionpediaDataset'\ncfg.data_root = train_images_path\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T15:37:23.090772Z","iopub.execute_input":"2024-03-28T15:37:23.091180Z","iopub.status.idle":"2024-03-28T15:37:23.133249Z","shell.execute_reply.started":"2024-03-28T15:37:23.091149Z","shell.execute_reply":"2024-03-28T15:37:23.132114Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmcv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmseg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_random_seed\n\u001b[1;32m      4\u001b[0m cfg \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mfromfile(config_file)\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'Config' from 'mmcv' (/opt/conda/lib/python3.10/site-packages/mmcv/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'Config' from 'mmcv' (/opt/conda/lib/python3.10/site-packages/mmcv/__init__.py)","output_type":"error"}]},{"cell_type":"code","source":" # Release memory\ninputs = None\nmasks = None\noutputs = None\nloss = None\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntry:\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        for batch_idx, sample_batched in enumerate(train_loader):\n            inputs = sample_batched['image'].to(DEVICE)\n            masks = {class_id: mask.to(DEVICE) for class_id, mask in sample_batched['masks'].items()}\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            outputs = F.interpolate(outputs, size=(IMAGE_HEIGHT, IMAGE_WIDTH), mode='bilinear', align_corners=False)\n\n\n            target_masks = torch.stack([masks[class_id].squeeze(1) for class_id in range(num_classes)], dim=1)\n            \n            loss = criterion(outputs, target_masks)\n\n            loss.backward()\n\n            optimizer.step()\n\n            # Print training statistics\n            if batch_idx % log_interval == 0:\n                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    epoch, batch_idx * len(inputs), len(train_loader.dataset),\n                    100. * batch_idx / len(train_loader), loss.item()))\n                \n            # Release memory\n            inputs = None\n            masks = None\n            outputs = None\n            torch.cuda.empty_cache()\nexcept Exception as e:\n    loss = None\n    inputs = None\n    masks = None\n    outputs = None\n    torch.cuda.empty_cache()\n    raise e","metadata":{"execution":{"iopub.status.busy":"2024-03-29T19:08:01.588867Z","iopub.execute_input":"2024-03-29T19:08:01.589985Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Train Epoch: 0 [0/45623 (0%)]\tLoss: 0.682694\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test results\n\nTesting the results obtained on a trained model","metadata":{}},{"cell_type":"code","source":"visualization_output_directory = working_dir + \"/visualization_results\"\nos.makedirs(visualization_output_directory, exist_ok=True) \n\ntest_images_path = root + \"/test\"\nfor index, row in test_images.iterrows():\n    image_path = os.path.join(test_images_path, row[\"file_name\"])\n    inference_and_visualize(model, image_path, visualization_output_directory)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot some sample images","metadata":{}},{"cell_type":"code","source":"samples = 1\ntrain_images_path = os.path.join(root, \"train\")\n\nfor _ in range(samples):\n    random_id = relevant_training_data.sample().index[0]\n    print(\"Image ID:\", random_id)\n    image_path = os.path.join(train_images_path, relevant_training_data.iloc[random_id][\"file_name\"])\n    print(\"Image path:\", image_path)\n    image_info = relevant_training_data.iloc[random_id]\n    print(\"Image info:\", image_info)\n\n    plot_raw_segmented_image(image_info, image_path)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}