{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7943996,"sourceType":"datasetVersion","datasetId":4670796},{"sourceId":7960409,"sourceType":"datasetVersion","datasetId":4682715}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install MMSegmentation¶","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-26T06:59:36.894405Z","iopub.execute_input":"2024-03-26T06:59:36.895263Z"}}},{"cell_type":"code","source":"!pip install torch==2.0.0\n!pip install openmim","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:02:52.719013Z","iopub.execute_input":"2024-03-27T22:02:52.719406Z","iopub.status.idle":"2024-03-27T22:06:16.014336Z","shell.execute_reply.started":"2024-03-27T22:02:52.719374Z","shell.execute_reply":"2024-03-27T22:06:16.013087Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torch==2.0.0\n  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0) (3.1.2)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.0.0 (from torch==2.0.0)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (69.0.3)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.42.0)\nCollecting cmake (from triton==2.0.0->torch==2.0.0)\n  Downloading cmake-3.29.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\nCollecting lit (from triton==2.0.0->torch==2.0.0)\n  Downloading lit-18.1.2.tar.gz (161 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.0/161.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.0) (1.3.0)\nDownloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cmake-3.29.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: lit\n  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-18.1.2-py3-none-any.whl size=96368 sha256=8b5bb22299f85fd34696ebfdf8601cb26d3ff054d242e1af51d5c725aa662f44\n  Stored in directory: /root/.cache/pip/wheels/f4/4d/9c/3e28d23c2c6fc6a9bd89c91a7b7ff775fc71a41ac9a52563e9\nSuccessfully built lit\nInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, cmake, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\nSuccessfully installed cmake-3.29.0.1 lit-18.1.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 triton-2.0.0\nCollecting openmim\n  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: Click in /opt/conda/lib/python3.10/site-packages (from openmim) (8.1.7)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from openmim) (0.4.6)\nCollecting model-index (from openmim)\n  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\nCollecting opendatalab (from openmim)\n  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from openmim) (2.1.4)\nRequirement already satisfied: pip>=19.3 in /opt/conda/lib/python3.10/site-packages (from openmim) (23.3.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from openmim) (2.31.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from openmim) (13.7.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from openmim) (0.9.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from model-index->openmim) (6.0.1)\nRequirement already satisfied: markdown in /opt/conda/lib/python3.10/site-packages (from model-index->openmim) (3.5.2)\nRequirement already satisfied: ordered-set in /opt/conda/lib/python3.10/site-packages (from model-index->openmim) (4.1.0)\nRequirement already satisfied: pycryptodome in /opt/conda/lib/python3.10/site-packages (from opendatalab->openmim) (3.20.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from opendatalab->openmim) (4.66.1)\nCollecting openxlab (from opendatalab->openmim)\n  Downloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->openmim) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->openmim) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->openmim) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->openmim) (2024.2.2)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas->openmim) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->openmim) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->openmim) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->openmim) (2023.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->openmim) (1.16.0)\nCollecting oss2~=2.17.0 (from openxlab->opendatalab->openmim)\n  Downloading oss2-2.17.0.tar.gz (259 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting requests (from openmim)\n  Downloading requests-2.28.2-py3-none-any.whl.metadata (4.6 kB)\nCollecting rich (from openmim)\n  Downloading rich-13.4.2-py3-none-any.whl.metadata (18 kB)\nCollecting setuptools~=60.2.0 (from openxlab->opendatalab->openmim)\n  Downloading setuptools-60.2.0-py3-none-any.whl.metadata (5.1 kB)\nCollecting tqdm (from opendatalab->openmim)\n  Downloading tqdm-4.65.2-py3-none-any.whl.metadata (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: crcmod>=1.7 in /opt/conda/lib/python3.10/site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (1.7)\nCollecting aliyun-python-sdk-kms>=2.4.1 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n  Downloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting aliyun-python-sdk-core>=2.13.12 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n  Downloading aliyun-python-sdk-core-2.15.0.tar.gz (443 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.1/443.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim)\n  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: cryptography>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (41.0.7)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.21)\nDownloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading model_index-0.1.11-py3-none-any.whl (34 kB)\nDownloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\nDownloading openxlab-0.0.37-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests-2.28.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rich-13.4.2-py3-none-any.whl (239 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm-4.65.2-py3-none-any.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading setuptools-60.2.0-py3-none-any.whl (953 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.1/953.1 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: oss2, aliyun-python-sdk-core\n  Building wheel for oss2 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for oss2: filename=oss2-2.17.0-py3-none-any.whl size=112371 sha256=8edeb5367e57e171438c0f00801e544e9ac7f0ea6000fb9c56872b90f410415c\n  Stored in directory: /root/.cache/pip/wheels/87/04/7b/7e61b8157fdf211c5131375240d0d86ca82e2a88ead9672c88\n  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.15.0-py3-none-any.whl size=535312 sha256=42441f58030a78d50b0db3c4120f9e094d84b44b3551231d4a1c5026ac138d2b\n  Stored in directory: /root/.cache/pip/wheels/b7/28/7c/a888bb3c60c865d014c7ef5017c83fdbc1cb0f601b79c7794a\nSuccessfully built oss2 aliyun-python-sdk-core\nInstalling collected packages: tqdm, setuptools, requests, model-index, jmespath, rich, aliyun-python-sdk-core, aliyun-python-sdk-kms, oss2, openxlab, opendatalab, openmim\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.66.1\n    Uninstalling tqdm-4.66.1:\n      Successfully uninstalled tqdm-4.66.1\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 69.0.3\n    Uninstalling setuptools-69.0.3:\n      Successfully uninstalled setuptools-69.0.3\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: jmespath\n    Found existing installation: jmespath 1.0.1\n    Uninstalling jmespath-1.0.1:\n      Successfully uninstalled jmespath-1.0.1\n  Attempting uninstall: rich\n    Found existing installation: rich 13.7.0\n    Uninstalling rich-13.7.0:\n      Successfully uninstalled rich-13.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.5 which is incompatible.\nboto3 1.26.100 requires botocore<1.30.0,>=1.29.100, but you have botocore 1.34.51 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2024.3.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-pubsub 2.19.0 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\njupyterlab-server 2.25.2 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aliyun-python-sdk-core-2.15.0 aliyun-python-sdk-kms-2.16.2 jmespath-0.10.0 model-index-0.1.11 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.37 oss2-2.17.0 requests-2.28.2 rich-13.4.2 setuptools-60.2.0 tqdm-4.65.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!mim install mmengine\n!mim install mmcv>=2.0.0\n!pip install mmsegmentation>=1.0.0\n!pip install ftfy ","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:07:15.262492Z","iopub.execute_input":"2024-03-27T22:07:15.263346Z","iopub.status.idle":"2024-03-27T22:09:28.396101Z","shell.execute_reply.started":"2024-03-27T22:07:15.263305Z","shell.execute_reply":"2024-03-27T22:09:28.394936Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n  warnings.warn(\nLooking in links: https://download.openmmlab.com/mmcv/dist/cu117/torch2.0.0/index.html\nCollecting mmengine\n  Downloading mmengine-0.10.3-py3-none-any.whl.metadata (20 kB)\nCollecting addict (from mmengine)\n  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mmengine) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mmengine) (1.26.4)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from mmengine) (6.0.1)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from mmengine) (13.4.2)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from mmengine) (2.4.0)\nRequirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from mmengine) (0.40.2)\nRequirement already satisfied: opencv-python>=3 in /opt/conda/lib/python3.10/site-packages (from mmengine) (4.9.0.80)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine) (2.9.0.post0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->mmengine) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->mmengine) (2.17.2)\nRequirement already satisfied: importlib-metadata>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine) (6.11.0)\nRequirement already satisfied: platformdirs>=3.5.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine) (4.2.0)\nRequirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine) (2.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.6.0->yapf->mmengine) (3.17.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->mmengine) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mmengine) (1.16.0)\nDownloading mmengine-0.10.3-py3-none-any.whl (451 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.7/451.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nInstalling collected packages: addict, mmengine\nSuccessfully installed addict-2.4.0 mmengine-0.10.3\n/opt/conda/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n  warnings.warn(\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.0 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-server 2.25.2 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nopentelemetry-api 1.22.0 requires importlib-metadata<7.0,>=6.0, but you have importlib-metadata 7.1.0 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\nrmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorstore 0.1.56 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nvirtualenv 20.21.0 requires platformdirs<4,>=2.4, but you have platformdirs 4.2.0 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting ftfy\n  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\nDownloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# !git clone -b main https://github.com/open-mmlab/mmsegmentation.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !cd mmsegmentation && pip install -e .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mim download mmsegmentation --config ocrnet_hr18_4xb2-160k_cityscapes-512x1024 --dest .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport mmseg\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mmseg.apis import inference_model, init_model, show_result_pyplot\nimport mmcv\n\nconfig_file = '/kaggle/working/ocrnet_hr18_4xb2-160k_cityscapes-512x1024.py'\ncheckpoint_file = '/kaggle/working/ocrnet_hr18_512x1024_160k_cityscapes_20200602_191001-b9172d0c.pth'\n\n# build the model from a config file and a checkpoint file\nmodel = init_model(config_file, checkpoint_file, device='cpu')\n\n# test a single image and show the results\nimg = '/kaggle/working/mmsegmentation/demo/demo.png'  # or img = mmcv.imread(img), which will only load it once\nresult = inference_model(model, img)\n# visualize the results in a new window\nshow_result_pyplot(model, img, result, show=True)\n# or save the visualization results to image files\n# you can change the opacity of the painted segmentation map in (0, 1].\nshow_result_pyplot(model, img, result, show=True, out_file='result.jpg', opacity=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and prepare data for training","metadata":{}},{"cell_type":"code","source":"from glob import glob\nimport cv2\nimport numpy as np\nimport xml.etree.ElementTree as ET\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nsns.set_theme()\nimport pandas as pd\nimport os\nimport json\n\n# ================================================================================================\n\nimport platform\nimport sys","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:09:42.325850Z","iopub.execute_input":"2024-03-27T22:09:42.326307Z","iopub.status.idle":"2024-03-27T22:09:44.235341Z","shell.execute_reply.started":"2024-03-27T22:09:42.326267Z","shell.execute_reply":"2024-03-27T22:09:44.234362Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# !mkdir /kaggle/working/OR_lab2/_models\n# !mkdir /kaggle/working/OR_lab2/_figures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---------------------------------------------------------------------------\nroot = \"/kaggle/input/fashionpedia-or/fashionpedia\"\nworking_dir = \"/kaggle/working\"\n# ---------------------------------------------------------------------------\n# Set models and figures directories\nmodels_dir = \"/kaggle/working/OR_lab2/_models\"\nfigures_dir = \"/kaggle/working/OR_lab2/_figures\"\n# ---------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:09:44.745586Z","iopub.execute_input":"2024-03-27T22:09:44.746794Z","iopub.status.idle":"2024-03-27T22:09:44.752200Z","shell.execute_reply.started":"2024-03-27T22:09:44.746756Z","shell.execute_reply":"2024-03-27T22:09:44.751239Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def load_annotations(annotation_file):\n    with open(annotation_file) as f:\n        data = json.load(f)\n        \n    images = pd.DataFrame(data['images'])\n    annotations = pd.DataFrame(data['annotations'])\n    categories = pd.DataFrame(data['categories'])\n    \n    return images, annotations, categories\n\ndef aggregate_rellevant_training_data(group):\n    category_ids = []\n    segmentations = []\n    file_name = \"\"\n    \n    for _, row in group.iterrows():\n        category_ids.append(row['category_id'])\n        segmentation = row['segmentation']\n        if isinstance(segmentation, list):  # If polygon encoded\n            segmentations.extend(segmentation)\n        elif isinstance(segmentation, dict):  # If RLE encoded\n            segmentations.append(segmentation)\n        file_name = row['file_name']\n\n    result_df = pd.Series({'image_id': group['image_id'].iloc[0], 'categories_ids': category_ids, 'segmentations': segmentations, 'file_name': file_name})\n    return result_df\n\ndef plot_raw_segmented_image(image_info, image_path, figsize=(15, 15)):\n    image = cv2.imread(image_path)\n    masked_image = image.copy()\n\n    encoded_pixels = image_info[\"segmentations\"]\n    class_ids = image_info[\"categories_ids\"]\n\n    # Plot the original image\n    fig, axs = plt.subplots(1, 2, figsize=figsize)\n    axs[0].imshow(image)\n\n    mask = np.zeros_like(image[:, :, 0])\n    for pixels, class_id in zip(encoded_pixels, class_ids):\n        # Create a mask for the current segment\n        vertices = np.array(pixels).reshape((-1, 2)).astype(np.int32)\n        cv2.fillPoly(mask, [vertices], 255 - class_id * 4)\n\n    axs[1].imshow(image)\n    axs[1].imshow(mask, alpha=0.8)\n\n    plt.show()\n    \n    \ndef save_raw_segmented_image(image_info, image_path, output_dir):\n    image = cv2.imread(image_path)\n    masked_image = image.copy()\n\n    encoded_pixels = image_info[\"segmentations\"]\n    class_ids = image_info[\"categories_ids\"]\n\n    mask = np.zeros_like(image[:, :, 0])\n    for pixels, class_id in zip(encoded_pixels, class_ids):\n        # Create a mask for the current segment\n        vertices = np.array(pixels).reshape((-1, 2)).astype(np.int32)\n        cv2.fillPoly(mask, [vertices], 255 - class_id * 4)\n\n    # Apply the mask to the image\n    masked_image[mask > 0] = [0, 255, 0]  # Set mask region to green (you can change color if needed)\n\n    # Save the masked image\n    filename = os.path.basename(image_path)\n    output_path = os.path.join(output_dir, filename)\n    cv2.imwrite(output_path, masked_image)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:09:51.675688Z","iopub.execute_input":"2024-03-27T22:09:51.676693Z","iopub.status.idle":"2024-03-27T22:09:51.694072Z","shell.execute_reply.started":"2024-03-27T22:09:51.676653Z","shell.execute_reply":"2024-03-27T22:09:51.692747Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(image_path):\n    train_transforms = transforms.Compose([\n        transforms.RandomRotation(degrees=20),  # Random rotation (±20 degrees)\n        transforms.RandomHorizontalFlip(),      # Random horizontal flip\n        transforms.RandomVerticalFlip(),        # Random vertical flip\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), \n        transforms.RandomResizedCrop(size=(height, width), scale=(0.8, 1.0)),  # Random resized crop\n        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=0.1),  # Random affine transformation\n        transforms.ToTensor(),                  # Convert image to tensor\n    ])\n    \n    img = mmcv.imread(image_path)\n    img = train_transforms(img)\n    return img\n\ndef inference_and_visualize(model, image_path, output_dir):\n    img = preprocess_image(image_path)\n    result = inference_model(model, img)\n    show_result_pyplot(model, img, result, show=False, out_file=os.path.join(output_dir, 'result.jpg'))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:09:54.144015Z","iopub.execute_input":"2024-03-27T22:09:54.144846Z","iopub.status.idle":"2024-03-27T22:09:54.154256Z","shell.execute_reply.started":"2024-03-27T22:09:54.144804Z","shell.execute_reply":"2024-03-27T22:09:54.153144Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_images, train_annotations, train_categories = load_annotations(root + \"/Annotations/instances_attributes_train2020.json\")\ntest_images, test_annotations, test_categories = load_annotations(root + \"/Annotations/instances_attributes_val2020.json\")\n\ntrain_merged_df = pd.merge(train_annotations, train_images, left_on='image_id', right_on='id', how='outer')\ntrain_merged_df = train_merged_df.drop(columns=['id_x', 'id_y', 'license', 'time_captured', 'isstatic', 'original_url', 'iscrowd', 'kaggle_id'])\n\n\nrelevant_training_data = train_merged_df.groupby('image_id').apply(aggregate_rellevant_training_data)\nrelevant_training_data.reset_index(drop=True, inplace=True)\n\nrelevant_training_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:09:56.478820Z","iopub.execute_input":"2024-03-27T22:09:56.479225Z","iopub.status.idle":"2024-03-27T22:11:01.592942Z","shell.execute_reply.started":"2024-03-27T22:09:56.479192Z","shell.execute_reply":"2024-03-27T22:11:01.591706Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/28044364.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  relevant_training_data = train_merged_df.groupby('image_id').apply(aggregate_rellevant_training_data)\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   image_id                                     categories_ids  \\\n0        23                                   [23, 23, 33, 10]   \n1        25             [2, 33, 31, 31, 13, 7, 22, 22, 23, 23]   \n2        26  [13, 29, 28, 32, 32, 31, 31, 0, 31, 31, 18, 4,...   \n3        27  [6, 23, 23, 31, 31, 4, 1, 35, 32, 35, 35, 35, ...   \n4        28                        [10, 32, 35, 31, 4, 29, 33]   \n\n                                       segmentations  \\\n0  [[456, 970, 465, 979, 483, 982, 501, 983, 503,...   \n1  [[391, 233, 393, 233, 397, 233, 398, 233, 400,...   \n2  [[441, 135, 456, 139, 457, 143, 462, 148, 472,...   \n3  [[317, 752, 324, 755, 330, 755, 334, 754, 340,...   \n4  [[428, 1008, 420, 1016, 413, 1020, 406, 1022, ...   \n\n                              file_name  \n0  3ce385855f07c77fdeb911ed15094c53.jpg  \n1  97e45101f7235a9e56fa95c5e4980c17.jpg  \n2  47cbe3ead1617a9971dccc438a8e8884.jpg  \n3  361cc7654672860b1b7c85fe8e92b38a.jpg  \n4  8a20effd8b6ebcaf2b74caa7d35eee41.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>categories_ids</th>\n      <th>segmentations</th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23</td>\n      <td>[23, 23, 33, 10]</td>\n      <td>[[456, 970, 465, 979, 483, 982, 501, 983, 503,...</td>\n      <td>3ce385855f07c77fdeb911ed15094c53.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25</td>\n      <td>[2, 33, 31, 31, 13, 7, 22, 22, 23, 23]</td>\n      <td>[[391, 233, 393, 233, 397, 233, 398, 233, 400,...</td>\n      <td>97e45101f7235a9e56fa95c5e4980c17.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>[13, 29, 28, 32, 32, 31, 31, 0, 31, 31, 18, 4,...</td>\n      <td>[[441, 135, 456, 139, 457, 143, 462, 148, 472,...</td>\n      <td>47cbe3ead1617a9971dccc438a8e8884.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27</td>\n      <td>[6, 23, 23, 31, 31, 4, 1, 35, 32, 35, 35, 35, ...</td>\n      <td>[[317, 752, 324, 755, 330, 755, 334, 754, 340,...</td>\n      <td>361cc7654672860b1b7c85fe8e92b38a.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>[10, 32, 35, 31, 4, 29, 33]</td>\n      <td>[[428, 1008, 420, 1016, 413, 1020, 406, 1022, ...</td>\n      <td>8a20effd8b6ebcaf2b74caa7d35eee41.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# No es necesari ja qu es fa on the fly\n# output_directory = root + \"/masked_images\"\n# os.makedirs(output_directory, exist_ok=True)\n\n# train_images_path = root + \"/train\"\n# relevant_training_data.apply(lambda row: save_masked_image(row, os.path.join(train_images_path, row[\"file_name\"]), output_directory), axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom mmseg.apis import inference_model, init_model, show_result_pyplot\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nimport mmcv\n","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:11:08.627754Z","iopub.execute_input":"2024-03-27T22:11:08.628719Z","iopub.status.idle":"2024-03-27T22:11:16.426374Z","shell.execute_reply.started":"2024-03-27T22:11:08.628683Z","shell.execute_reply":"2024-03-27T22:11:16.425360Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n/opt/conda/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"classes = ['pocket' 'sleeve' 'sock' 'collar' 'shirt, blouse' 'dress' 'shoe'\n 'tights, stockings' 'glasses' 'belt' 'bag, wallet' 'neckline' 'pants'\n 'hood' 'epaulette' 'coat' 'top, t-shirt, sweatshirt' 'zipper' 'hat'\n 'lapel' 'jacket' 'flower' 'ruffle' 'applique' 'skirt' 'buckle' 'scarf'\n 'glove' 'shorts' 'jumpsuit' 'bead' 'watch' 'tie'\n 'headband, head covering, hair accessory' 'umbrella' 'fringe' 'rivet'\n 'sweater' 'cardigan' 'vest' 'sequin' 'ribbon' 'bow' 'cape' 'tassel'\n 'leg warmer']\npalette = [255, 251, 247, 243, 239, 235, 231, 227, 223, 219, 215, 211, 207, 203, 199, 195, 191, 187, 183, 179, 175, 171, 167, 163, 159, 155, 151, 147, 143, 139, 135, 131, 127, 123, 119, 115, 111, 107, 103, 99, 95, 91, 87, 83, 79, 75]\n\n\ndef decode_rle(rle):\n    \"\"\"\n    Decode a run-length encoded mask.\n    \"\"\"\n    try:\n        h, w = rle[\"size\"]\n        mask = np.zeros(h * w, dtype=np.uint8)\n        counts = rle[\"counts\"]\n        counts = counts.encode('utf-8')  # Encoding the string to bytes for correct parsing\n        counts = np.frombuffer(counts, dtype=np.uint8) - 48  # Convert ASCII codes to integer values\n   \n        starts = np.cumsum(counts[::2])\n        \n        min_length = min(len(starts), len(counts[1::2]))\n        starts = starts[:min_length]\n        \n        ends = starts + counts[1::2]\n        \n        ends = ends[:min_length]\n        \n        for start, end in zip(starts, ends):\n            mask[start:end] = 1\n        mask = mask.reshape((h, w), order='F')\n        return mask\n    except Exception as e:\n        print(f\"Error decoding RLE: {str(e)}\")\n        raise e\n\nclass CustomDataset(torch.utils.data.Dataset):\n    CLASSES = classes\n    PALETTE = palette\n    \n    def __init__(self, data_df, root_dir, transform=None):\n        self.data_df = data_df\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data_df)\n\n    def __getitem__(self, idx):\n        try:\n            image_info = self.data_df.iloc[idx]\n            image_path = os.path.join(self.root_dir, image_info['file_name'])\n            image = cv2.imread(image_path)\n            encoded_segments = image_info[\"segmentations\"]\n            class_ids = image_info[\"categories_ids\"]\n            mask = np.zeros_like(image[:, :, 0])\n            for segment, class_id in zip(encoded_segments, class_ids):\n                if isinstance(segment, dict):  # If RLE encoded\n                    segment = decode_rle(segment)\n                else:  # If polygon encoded\n                    segment = np.array(segment).reshape((-1, 2)).astype(np.int32)\n                    cv2.fillPoly(mask, [segment], 255 - class_id * 4)\n\n            sample = {'image': image, 'mask': mask}\n\n            if self.transform:\n                sample = self.transform(sample)\n\n            return sample\n        except Exception as e:\n            print(\"Encoded_seg: \", segment)\n            print(f\"Error processing item {idx}: {str(e)}\")\n            raise Exception\n\n            \n    \nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image, mask = sample['image'], sample['mask']\n\n        # Swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        mask = mask.transpose((0, 1))\n\n        return {'image': torch.from_numpy(image),\n                'mask': torch.from_numpy(mask)}\n\nclass ApplyTransform(object):\n    def __init__(self, transform):\n        self.transform = transform\n\n    def __call__(self, sample):\n        image, mask = sample['image'], sample['mask']\n\n        # Apply transformation to both image and mask\n        image = self.transform(image)\n        mask = self.transform(mask)\n\n        # Ensure that the mask has a single channel\n        if mask.shape[0] > 1:\n            mask = mask[0]  # Take the first channel if there are multiple channels\n\n        return {'image': image, 'mask': mask}","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:11:18.949207Z","iopub.execute_input":"2024-03-27T22:11:18.949617Z","iopub.status.idle":"2024-03-27T22:11:18.974275Z","shell.execute_reply.started":"2024-03-27T22:11:18.949581Z","shell.execute_reply":"2024-03-27T22:11:18.972612Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntrain_images_path = root + \"/train\"\nbatch_size = 32\nnum_epochs = 8\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nIMAGE_HEIGHT = 192\nIMAGE_WIDTH = 192\n\n# Define transformations\ntrain_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomRotation(degrees=20),\n    transforms.RandomResizedCrop(size=(IMAGE_HEIGHT, IMAGE_WIDTH), scale=(0.8, 1.0)),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n    transforms.ToTensor(),\n])\n\n\ntrain_dataset = CustomDataset(data_df=relevant_training_data,\n                              root_dir=train_images_path,\n                              transform=ApplyTransform(train_transforms))","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:11:35.157354Z","iopub.execute_input":"2024-03-27T22:11:35.157778Z","iopub.status.idle":"2024-03-27T22:11:35.168411Z","shell.execute_reply.started":"2024-03-27T22:11:35.157745Z","shell.execute_reply":"2024-03-27T22:11:35.166947Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# config_file = working_dir + '/ocrnet_hr18_4xb2-160k_cityscapes-512x1024.py'\nconfig_file = \"/kaggle/input/ocrnet-hr18-4xb2-160k-cityscapes-512x1024/ocrnet_hr18_4xb2-160k_cityscapes-512x1024.py\"\ncheckpoint_file = working_dir +'/ocrnet_hr18_512x1024_160k_cityscapes_20200602_191001-b9172d0c.pth'\n\nmodel = init_model(config_file, checkpoint_file, device=DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters())\nlog_interval = 10","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:27:15.498227Z","iopub.execute_input":"2024-03-27T22:27:15.498657Z","iopub.status.idle":"2024-03-27T22:27:16.174954Z","shell.execute_reply.started":"2024-03-27T22:27:15.498622Z","shell.execute_reply":"2024-03-27T22:27:16.173950Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/mmseg/models/builder.py:36: UserWarning: ``build_loss`` would be deprecated soon, please use ``mmseg.registry.MODELS.build()`` \n  warnings.warn('``build_loss`` would be deprecated soon, please use '\n/opt/conda/lib/python3.10/site-packages/mmseg/models/losses/cross_entropy_loss.py:250: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Loads checkpoint by local backend from path: /kaggle/working/ocrnet_hr18_512x1024_160k_cityscapes_20200602_191001-b9172d0c.pth\nThe model and loaded state dict do not match exactly\n\nsize mismatch for decode_head.0.conv_seg.weight: copying a param with shape torch.Size([19, 270, 1, 1]) from checkpoint, the shape in current model is torch.Size([46, 270, 1, 1]).\nsize mismatch for decode_head.0.conv_seg.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([46]).\nsize mismatch for decode_head.1.conv_seg.weight: copying a param with shape torch.Size([19, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([46, 512, 1, 1]).\nsize mismatch for decode_head.1.conv_seg.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([46]).\n","output_type":"stream"}]},{"cell_type":"code","source":"from mmcv import Config\nfrom mmseg.apis import set_random_seed\n\ncfg = Config.fromfile(config_file)\n\n\ncfg.norm_cfg = dict(type='BN', requires_grad=True)\ncfg.model.backbone.norm_cfg = cfg.norm_cfg\ncfg.model.decode_head.norm_cfg = cfg.norm_cfg\ncfg.model.auxiliary_head.norm_cfg = cfg.norm_cfg\n\ncfg.model.decode_head.num_classes = 46\ncfg.model.auxiliary_head.num_classes = 46\n\ncfg.dataset_type = 'CustomDataset'\ncfg.data_root = train_images_path\n","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:19:39.339469Z","iopub.execute_input":"2024-03-27T22:19:39.340364Z","iopub.status.idle":"2024-03-27T22:19:39.387672Z","shell.execute_reply.started":"2024-03-27T22:19:39.340331Z","shell.execute_reply":"2024-03-27T22:19:39.386294Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmcv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmseg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_random_seed\n\u001b[1;32m      4\u001b[0m cfg \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mfromfile(config_file)\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'Config' from 'mmcv' (/opt/conda/lib/python3.10/site-packages/mmcv/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'Config' from 'mmcv' (/opt/conda/lib/python3.10/site-packages/mmcv/__init__.py)","output_type":"error"}]},{"cell_type":"code","source":" # Release memory\ninputs = None\nmasks = None\noutputs = None\nloss = None\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntry:\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        for batch_idx, sample_batched in enumerate(train_loader):\n            inputs, masks = sample_batched['image'].to(DEVICE), sample_batched['mask'].to(DEVICE)\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n\n            masks = masks.squeeze(1)\n            outputs = outputs.squeeze(1)\n\n            print(masks.shape)\n            print(outputs.shape)\n\n            loss = criterion(outputs, masks)\n\n            loss.backward()\n\n            optimizer.step()\n\n             # Release memory\n            inputs = None\n            masks = None\n            outputs = None\n            torch.cuda.empty_cache()\n\n            # Print training statistics\n            if batch_idx % log_interval == 0:\n                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    epoch, batch_idx * len(inputs), len(train_loader.dataset),\n                    100. * batch_idx / len(train_loader), loss.item()))\nexcept Exception as e:\n    loss = None\n    inputs = None\n    masks = None\n    outputs = None\n    torch.cuda.empty_cache()\n    raise e\n","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:27:44.954069Z","iopub.execute_input":"2024-03-27T22:27:44.954470Z","iopub.status.idle":"2024-03-27T22:27:49.514619Z","shell.execute_reply.started":"2024-03-27T22:27:44.954439Z","shell.execute_reply":"2024-03-27T22:27:49.512940Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"torch.Size([32, 192, 192])\ntorch.Size([32, 46, 48, 48])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     40\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n","Cell \u001b[0;32mIn[28], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(masks\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: input and target batch or spatial sizes don't match: target [32, 192, 192], input [32, 46, 48, 48]"],"ename":"RuntimeError","evalue":"input and target batch or spatial sizes don't match: target [32, 192, 192], input [32, 46, 48, 48]","output_type":"error"}]},{"cell_type":"markdown","source":"# Test results\n\nTesting the results obtained on a trained model","metadata":{}},{"cell_type":"code","source":"visualization_output_directory = working_dir + \"/visualization_results\"\nos.makedirs(visualization_output_directory, exist_ok=True) \n\ntest_images_path = root + \"/test\"\nfor index, row in test_images.iterrows():\n    image_path = os.path.join(test_images_path, row[\"file_name\"])\n    inference_and_visualize(model, image_path, visualization_output_directory)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot some sample images","metadata":{}},{"cell_type":"code","source":"samples = 1\ntrain_images_path = os.path.join(root, \"train\")\n\nfor _ in range(samples):\n    random_id = relevant_training_data.sample().index[0]\n    print(\"Image ID:\", random_id)\n    image_path = os.path.join(train_images_path, relevant_training_data.iloc[random_id][\"file_name\"])\n    print(\"Image path:\", image_path)\n    image_info = relevant_training_data.iloc[random_id]\n    print(\"Image info:\", image_info)\n\n    plot_raw_segmented_image(image_info, image_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}