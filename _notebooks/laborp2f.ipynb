{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18237,"databundleVersionId":1053191,"sourceType":"competition"},{"sourceId":7940913,"sourceType":"datasetVersion","datasetId":4668718}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install MMSegmentation","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -U openmim\n!mim install mmengine\n!mim install \"mmcv>=2.0.0\"\n!pip install \"mmsegmentation>=1.0.0\"\n!pip install ftfy","metadata":{"execution":{"iopub.status.busy":"2024-03-26T20:13:25.698463Z","iopub.execute_input":"2024-03-26T20:13:25.698896Z","iopub.status.idle":"2024-03-26T20:15:31.965738Z","shell.execute_reply.started":"2024-03-26T20:13:25.698862Z","shell.execute_reply":"2024-03-26T20:15:31.964111Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#%%capture\n#!git clone -b main https://github.com/open-mmlab/mmsegmentation.git\n#!cd mmsegmentation\n#!pip install -v -e .","metadata":{"execution":{"iopub.status.busy":"2024-03-26T20:15:31.968576Z","iopub.execute_input":"2024-03-26T20:15:31.969044Z","iopub.status.idle":"2024-03-26T20:15:31.974931Z","shell.execute_reply.started":"2024-03-26T20:15:31.968996Z","shell.execute_reply":"2024-03-26T20:15:31.973753Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Example","metadata":{}},{"cell_type":"code","source":"#!mim download mmsegmentation --config pspnet_r50-d8_4xb2-40k_cityscapes-512x1024 --dest .","metadata":{"execution":{"iopub.status.busy":"2024-03-26T20:15:31.976457Z","iopub.execute_input":"2024-03-26T20:15:31.976865Z","iopub.status.idle":"2024-03-26T20:15:42.298692Z","shell.execute_reply.started":"2024-03-26T20:15:31.976827Z","shell.execute_reply":"2024-03-26T20:15:42.297610Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n  warnings.warn(\nprocessing pspnet_r50-d8_4xb2-40k_cityscapes-512x1024...\n\u001b[32mpspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth exists in /kaggle/working\u001b[0m\n\u001b[32mSuccessfully dumped pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py to /kaggle/working\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from mmseg.apis import inference_model, init_model, show_result_pyplot\nimport mmcv\n\nconfig_file = '/kaggle/working/pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py'\ncheckpoint_file = '/kaggle/working/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'\n\n# build the model from a config file and a checkpoint file\nmodel = init_model(config_file, checkpoint_file, device='cpu')\n\n# test a single image and show the results\nimg = '/kaggle/working/mmsegmentation/demo/demo.png'  # or img = mmcv.imread(img), which will only load it once\nresult = inference_model(model, img)\n# visualize the results in a new window\nshow_result_pyplot(model, img, result, show=True)\n# or save the visualization results to image files\n# you can change the opacity of the painted segmentation map in (0, 1].\nshow_result_pyplot(model, img, result, show=True, out_file='result.jpg', opacity=0.5)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-26T20:15:42.302372Z","iopub.execute_input":"2024-03-26T20:15:42.303844Z","iopub.status.idle":"2024-03-26T20:16:33.136789Z","shell.execute_reply.started":"2024-03-26T20:15:42.303802Z","shell.execute_reply":"2024-03-26T20:16:33.135456Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/mmseg/models/builder.py:36: UserWarning: ``build_loss`` would be deprecated soon, please use ``mmseg.registry.MODELS.build()`` \n  warnings.warn('``build_loss`` would be deprecated soon, please use '\n/opt/conda/lib/python3.10/site-packages/mmseg/models/losses/cross_entropy_loss.py:250: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Loads checkpoint by local backend from path: /kaggle/working/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([[[ 87,  94,  87],\n        [ 69,  80,  74],\n        [ 94,  99,  99],\n        ...,\n        [ 86,  90,  80],\n        [ 88,  91,  81],\n        [ 86,  90,  80]],\n\n       [[ 89,  96,  88],\n        [ 73,  83,  76],\n        [ 94,  99,  99],\n        ...,\n        [ 86,  90,  80],\n        [ 88,  90,  79],\n        [ 86,  90,  80]],\n\n       [[ 87,  94,  87],\n        [ 73,  83,  76],\n        [ 94,  99,  99],\n        ...,\n        [ 86,  90,  80],\n        [ 86,  88,  78],\n        [ 86,  90,  80]],\n\n       ...,\n\n       [[ 88,  63,  91],\n        [ 88,  63,  91],\n        [ 88,  61,  90],\n        ...,\n        [102,  77, 102],\n        [100,  76, 101],\n        [102,  77, 102]],\n\n       [[ 88,  61,  90],\n        [ 88,  63,  91],\n        [ 86,  61,  89],\n        ...,\n        [ 99,  73, 100],\n        [100,  74, 100],\n        [100,  74, 101]],\n\n       [[ 88,  61,  90],\n        [ 88,  63,  91],\n        [ 86,  61,  89],\n        ...,\n        [ 90,  60,  88],\n        [ 89,  62,  89],\n        [ 92,  65,  92]]], dtype=uint8)"},"metadata":{}}]},{"cell_type":"markdown","source":"## P2 SWIN","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom torchvision import transforms\n\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-26T20:58:00.559763Z","iopub.execute_input":"2024-03-26T20:58:00.560215Z","iopub.status.idle":"2024-03-26T20:58:00.566865Z","shell.execute_reply.started":"2024-03-26T20:58:00.560178Z","shell.execute_reply":"2024-03-26T20:58:00.565540Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"train_file = \"/kaggle/input/annotations-fashionpedia/instances_attributes_train2020.json\"","metadata":{"execution":{"iopub.status.busy":"2024-03-26T20:58:00.697385Z","iopub.execute_input":"2024-03-26T20:58:00.697840Z","iopub.status.idle":"2024-03-26T20:58:00.703571Z","shell.execute_reply.started":"2024-03-26T20:58:00.697805Z","shell.execute_reply":"2024-03-26T20:58:00.702244Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"def load_metadata(filename):\n    test_merged_df = None\n    with open(filename) as json_data:\n        data = json.load(json_data)\n        test_categories = pd.DataFrame(data['categories'])\n        test_img = pd.DataFrame(data['images'])\n        test_annotations = pd.DataFrame(data['annotations'])\n        test_merged_df = pd.merge(test_annotations, test_img, left_on='image_id', right_on='id', how='outer')\n        test_merged_df = test_merged_df.drop(columns=['id_x', 'id_y', 'license', 'time_captured', 'isstatic', 'original_url', 'iscrowd', 'kaggle_id'])\n        test_merged_df[\"img_area\"] = test_merged_df[\"height\"] * test_merged_df[\"width\"]\n        test_merged_df['area_ratio'] = test_merged_df['area'] / test_merged_df['img_area']\n    return test_merged_df","metadata":{"execution":{"iopub.status.busy":"2024-03-26T20:58:00.838656Z","iopub.execute_input":"2024-03-26T20:58:00.839075Z","iopub.status.idle":"2024-03-26T20:58:00.847848Z","shell.execute_reply.started":"2024-03-26T20:58:00.839039Z","shell.execute_reply":"2024-03-26T20:58:00.846701Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, metadata_file, split, transform=None, image_dir=\"/kaggle/input/imaterialist-fashion-2020-fgvc7\"):\n        self.metadata = load_metadata(metadata_file)\n        self.transform = transform\n        self.image_dir = os.path.join(image_dir, split)\n        \n        \n    def __len__(self):\n        return len(self.metadata)\n\n    def __getitem__(self, index):\n        filename = self.metadata['file_name'][index]\n        label = self.metadata['segmentation'][index]\n       \n        image_path = os.path.join(self.image_dir, filename)\n        image = Image.open(image_path).convert(\"RGB\")\n     \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:06:40.083618Z","iopub.execute_input":"2024-03-26T21:06:40.084043Z","iopub.status.idle":"2024-03-26T21:06:40.093617Z","shell.execute_reply.started":"2024-03-26T21:06:40.084000Z","shell.execute_reply":"2024-03-26T21:06:40.092328Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"def train(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)\n    return running_loss / len(train_loader.dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:06:40.478724Z","iopub.execute_input":"2024-03-26T21:06:40.479136Z","iopub.status.idle":"2024-03-26T21:06:40.486983Z","shell.execute_reply.started":"2024-03-26T21:06:40.479105Z","shell.execute_reply":"2024-03-26T21:06:40.485747Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"def validate(model, val_loader, criterion, device):\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item() * inputs.size(0)\n            \n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    accuracy = correct / total\n    return val_loss / len(val_loader.dataset), accuracy\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:06:40.630037Z","iopub.execute_input":"2024-03-26T21:06:40.630494Z","iopub.status.idle":"2024-03-26T21:06:40.639203Z","shell.execute_reply.started":"2024-03-26T21:06:40.630460Z","shell.execute_reply":"2024-03-26T21:06:40.638271Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"train_file = \"/kaggle/input/annotations-fashionpedia/instances_attributes_train2020.json\"\ntest_file = \"/kaggle/input/annotations-fashionpedia/instances_attributes_val2020.json\"","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:06:40.789328Z","iopub.execute_input":"2024-03-26T21:06:40.789766Z","iopub.status.idle":"2024-03-26T21:06:40.795710Z","shell.execute_reply.started":"2024-03-26T21:06:40.789736Z","shell.execute_reply":"2024-03-26T21:06:40.794245Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((512, 512)), \n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:06:40.952078Z","iopub.execute_input":"2024-03-26T21:06:40.952485Z","iopub.status.idle":"2024-03-26T21:06:40.959349Z","shell.execute_reply.started":"2024-03-26T21:06:40.952454Z","shell.execute_reply":"2024-03-26T21:06:40.957865Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(train_file, \"train\", transform=transform)\nval_dataset = CustomDataset(test_file, \"test\", transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:06:41.099171Z","iopub.execute_input":"2024-03-26T21:06:41.099610Z","iopub.status.idle":"2024-03-26T21:06:56.832077Z","shell.execute_reply.started":"2024-03-26T21:06:41.099576Z","shell.execute_reply":"2024-03-26T21:06:56.830749Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"config_file = '/kaggle/working/mmsegmentation/configs/swin/swin-base-patch4-window7-in1k-pre_upernet_8xb2-160k_ade20k-512x512.py'","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:06:56.834324Z","iopub.execute_input":"2024-03-26T21:06:56.834681Z","iopub.status.idle":"2024-03-26T21:06:56.841026Z","shell.execute_reply.started":"2024-03-26T21:06:56.834651Z","shell.execute_reply":"2024-03-26T21:06:56.839585Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nbatch_size = 32\nnum_epochs = 10\n\n\nmodel = init_model(config_file, device=device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001) \ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n\nfor epoch in range(num_epochs):\n    train_loss = train(model, train_loader, criterion, optimizer, device)\n    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n    \n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:06:56.842506Z","iopub.execute_input":"2024-03-26T21:06:56.842823Z","iopub.status.idle":"2024-03-26T21:07:01.838731Z","shell.execute_reply.started":"2024-03-26T21:06:56.842796Z","shell.execute_reply":"2024-03-26T21:07:01.835925Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/mmseg/models/builder.py:36: UserWarning: ``build_loss`` would be deprecated soon, please use ``mmseg.registry.MODELS.build()`` \n  warnings.warn('``build_loss`` would be deprecated soon, please use '\n/opt/conda/lib/python3.10/site-packages/mmseg/models/losses/cross_entropy_loss.py:250: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[130], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 15\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[124], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      5\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:138\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n","\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"],"ename":"RuntimeError","evalue":"each element in list of batch should be of equal size","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}